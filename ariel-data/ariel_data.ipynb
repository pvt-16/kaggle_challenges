{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pvt-16/kaggle-ariel-data-challenge/blob/master/ariel_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import itertools\n",
        "import glob\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-09-08T18:08:35.164849Z",
          "iopub.status.busy": "2024-09-08T18:08:35.164426Z",
          "iopub.status.idle": "2024-09-08T18:08:35.170378Z",
          "shell.execute_reply": "2024-09-08T18:08:35.168918Z",
          "shell.execute_reply.started": "2024-09-08T18:08:35.16481Z"
        },
        "id": "PpSMMwLX6w0w",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import dask.dataframe as dd\n",
        "from astropy.stats import sigma_clip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "CHUNKS_SIZE = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_directory = os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_folder =  os.path.join(current_directory, 'Dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_out = '/kaggle/tmp/data_light_raw/' # path to the folder to store the light data\n",
        "output_dir = '/kaggle/tmp/data_light_raw/' # path for the output directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read Metadata files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-08T18:08:36.70434Z",
          "iopub.status.busy": "2024-09-08T18:08:36.703428Z",
          "iopub.status.idle": "2024-09-08T18:08:36.709269Z",
          "shell.execute_reply": "2024-09-08T18:08:36.70798Z",
          "shell.execute_reply.started": "2024-09-08T18:08:36.704277Z"
        },
        "id": "3FKn_u206w0w",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "input_path = 'Dataset/Metadata/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels_df = pd.read_csv(f'{input_path}train_labels.csv')\n",
        "wavelengths = pd.read_csv(f'{input_path}wavelengths.csv')\n",
        "test_adc_info = pd.read_csv(f'{input_path}test_adc_info.csv')\n",
        "train_adc_info = pd.read_csv(f'{input_path}train_adc_info.csv')\n",
        "axis_info = pd.read_parquet(f'{input_path}axis_info.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Starter code at: https://www.kaggle.com/code/gordonyip/update-calibrating-and-binning-astronomical-data (versoin 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Analog-to-Digital Conversion\n",
        "\n",
        "The Analog-to-Digital Conversion (adc) is performed by the detector to convert the pixel voltage into an integer number. We revert this operation by using the gain and offset for the calibration files 'train_adc_info.csv'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ADC_convert(signal, gain, offset):\n",
        "    signal = signal.astype(np.float64)\n",
        "    signal /= gain\n",
        "    signal += offset\n",
        "    return signal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2.1: Mask hot/dead pixel\n",
        "The dead pixels map is a map of the pixels that do not respond to light and, thus, can’t be accounted for any calculation. In all these frames the dead pixels are masked using python masked arrays. The bad pixels are thus masked but left uncorrected. Some methods can be used to correct bad-pixels but this task, if needed, is left to the participants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mask_hot_dead(signal, dead, dark):\n",
        "    hot = sigma_clip(\n",
        "        dark, sigma=5, maxiters=5\n",
        "    ).mask\n",
        "    hot = np.tile(hot, (signal.shape[0], 1, 1))\n",
        "    dead = np.tile(dead, (signal.shape[0], 1, 1))\n",
        "    signal = np.ma.masked_where(dead, signal)\n",
        "    signal = np.ma.masked_where(hot, signal)\n",
        "    return signal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2.2: linearity Correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**Non-linearity of pixels' response:**\n",
        "\n",
        "The non-linearity of the pixels’ response can be explained as capacitive leakage on the readout electronics of each pixel during the integration time. The number of electrons in the well is proportional to the number of photons that hit the pixel, with a quantum efficiency coefficient. However, the response of the pixel is not linear with the number of electrons in the well. This effect can be described by a polynomial function of the number of electrons actually in the well. The data is provided with calibration files linear_corr.parquet that are the coefficients of the inverse polynomial function and can be used to correct this non-linearity effect.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_linear_corr(linear_corr,clean_signal):\n",
        "    linear_corr = np.flip(linear_corr, axis=0)\n",
        "    for x, y in itertools.product(\n",
        "                range(clean_signal.shape[1]), range(clean_signal.shape[2])\n",
        "            ):\n",
        "        poli = np.poly1d(linear_corr[:, x, y])\n",
        "        clean_signal[:, x, y] = poli(clean_signal[:, x, y])\n",
        "    return clean_signal\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: dark current subtraction\n",
        "\n",
        "The data provided include calibration for dark current estimation, which can be used to pre-process the observations. Dark current represents a constant signal that accumulates in each pixel during the integration time, independent of the incoming light. To obtain the corrected image, the following conventional approach is applied: The data provided include calibration files such as dark frames or dead pixels' maps. They can be used to pre-process the observations. The dark frame is a map of the detector response to a very short exposure time, to correct for the dark current of the detector.\n",
        "$$\\text{image - dark} \\times \\Delta t $$ \n",
        "The corrected image is conventionally obtained via the following: where the dark current map is first corrected for the dead pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_dark(signal, dead, dark, dt):\n",
        "\n",
        "    dark = np.ma.masked_where(dead, dark)\n",
        "    dark = np.tile(dark, (signal.shape[0], 1, 1))\n",
        "\n",
        "    signal -= dark* dt[:, np.newaxis, np.newaxis]\n",
        "    return signal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Get Correlated Double Sampling (CDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The science frames are alternating between the start of the exposure and the end of the exposure. The lecture scheme is a ramp with a double sampling, called Correlated Double Sampling (CDS), the detector is read twice, once at the start of the exposure and once at the end of the exposure. The final CDS is the difference (End of exposure) - (Start of exposure)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_cds(signal):\n",
        "    cds = signal[:,1::2,:,:] - signal[:,::2,:,:]\n",
        "    return cds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 (Optional): Time Binning\n",
        "This step is performed mianly to save space. Time series observations are binned together at specified frequency. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bin_obs(cds_signal,binning):\n",
        "    cds_transposed = cds_signal.transpose(0,1,3,2)\n",
        "    cds_binned = np.zeros((cds_transposed.shape[0], cds_transposed.shape[1]//binning, cds_transposed.shape[2], cds_transposed.shape[3]))\n",
        "    for i in range(cds_transposed.shape[1]//binning):\n",
        "        cds_binned[:,i,:,:] = np.sum(cds_transposed[:,i*binning:(i+1)*binning,:,:], axis=1)\n",
        "    return cds_binned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Flat Field Correction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The flat field is a map of the detector response to uniform illumination, to correct for the pixel-to-pixel variations of the detector, for example the different quantum efficiencies of each pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correct_flat_field(flat,dead, signal):\n",
        "    flat = flat.transpose(1, 0)\n",
        "    dead = dead.transpose(1, 0)\n",
        "    flat = np.ma.masked_where(dead, flat)\n",
        "    flat = np.tile(flat, (signal.shape[0], 1, 1))\n",
        "    signal = signal / flat\n",
        "    return signal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Calibrating all training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "you can choose to correct the non-linearity of the pixels' response, to apply flat field, dark and dead map or to leave the data unchanged. The observations are binned in time by group of 30 frames for AIRS and 360 frames for FGS1, to obtain a lighter data-cube, easier to use. The images are cut along the wavelength axis between pixels 39 and 321, so that the 282 pixels left in the wavelength dimension match the last 282 targets' points, from AIRS. The 283rd targets' point is the one for FGS1 that will be added later on. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "## we will start by getting the index of the training data:\n",
        "def get_index(files,CHUNKS_SIZE ):\n",
        "    index = []\n",
        "    for file in files :\n",
        "        file_name = file.split('/')[-1]\n",
        "        if file_name.split('_')[0] == 'AIRS-CH0' and file_name.split('_')[1] == 'signal.parquet':\n",
        "            file_index = os.path.basename(os.path.dirname(file))\n",
        "            index.append(int(file_index))\n",
        "    index = np.array(index)\n",
        "    index = np.sort(index) \n",
        "    # credit to DennisSakva\n",
        "    index=np.array_split(index, len(index)//CHUNKS_SIZE)\n",
        "    \n",
        "    return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "train_adc_info = pd.read_csv(os.path.join(path_folder, 'train_adc_info.csv'))\n",
        "train_adc_info = train_adc_info.set_index('planet_id')\n",
        "axis_info = pd.read_parquet(os.path.join(path_folder,'axis_info.parquet'))\n",
        "DO_MASK = True\n",
        "DO_THE_NL_CORR = False\n",
        "DO_DARK = True\n",
        "DO_FLAT = True\n",
        "TIME_BINNING = True\n",
        "\n",
        "cut_inf, cut_sup = 39, 321\n",
        "l = cut_sup - cut_inf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\ptv_m\\\\source\\\\repos\\\\kaggle-ariel-data-challenge\\\\Dataset'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "files = glob.glob(os.path.join(path_folder + 'train/', '*/*'))\n",
        "path_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "index = get_index(files[:22], CHUNKS_SIZE)  ## 48 is hardcoded here but please feel free to remove it if you want to do it for the entire dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calibrate_all_data():\n",
        "    for n, index_chunk in enumerate(tqdm(index)):\n",
        "        AIRS_CH0_clean = np.zeros((CHUNKS_SIZE, 11250, 32, l))\n",
        "        FGS1_clean = np.zeros((CHUNKS_SIZE, 135000, 32, 32))\n",
        "        \n",
        "        for i in range (CHUNKS_SIZE) : \n",
        "            df = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_signal.parquet'))\n",
        "            signal = df.values.astype(np.float64).reshape((df.shape[0], 32, 356))\n",
        "            gain = train_adc_info['AIRS-CH0_adc_gain'].loc[index_chunk[i]]\n",
        "            offset = train_adc_info['AIRS-CH0_adc_offset'].loc[index_chunk[i]]\n",
        "            signal = ADC_convert(signal, gain, offset)\n",
        "            dt_airs = axis_info['AIRS-CH0-integration_time'].dropna().values\n",
        "            dt_airs[1::2] += 0.1\n",
        "            chopped_signal = signal[:, :, cut_inf:cut_sup]\n",
        "            del signal, df\n",
        "            \n",
        "            # CLEANING THE DATA: AIRS\n",
        "            flat = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_calibration/flat.parquet')).values.astype(np.float64).reshape((32, 356))[:, cut_inf:cut_sup]\n",
        "            dark = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_calibration/dark.parquet')).values.astype(np.float64).reshape((32, 356))[:, cut_inf:cut_sup]\n",
        "            dead_airs = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_calibration/dead.parquet')).values.astype(np.float64).reshape((32, 356))[:, cut_inf:cut_sup]\n",
        "            linear_corr = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_calibration/linear_corr.parquet')).values.astype(np.float64).reshape((6, 32, 356))[:, :, cut_inf:cut_sup]\n",
        "            \n",
        "            if DO_MASK:\n",
        "                chopped_signal = mask_hot_dead(chopped_signal, dead_airs, dark)\n",
        "                AIRS_CH0_clean[i] = chopped_signal\n",
        "            else:\n",
        "                AIRS_CH0_clean[i] = chopped_signal\n",
        "                \n",
        "            if DO_THE_NL_CORR: \n",
        "                linear_corr_signal = apply_linear_corr(linear_corr,AIRS_CH0_clean[i])\n",
        "                AIRS_CH0_clean[i,:, :, :] = linear_corr_signal\n",
        "            del linear_corr\n",
        "            \n",
        "            if DO_DARK: \n",
        "                cleaned_signal = clean_dark(AIRS_CH0_clean[i], dead_airs, dark, dt_airs)\n",
        "                AIRS_CH0_clean[i] = cleaned_signal\n",
        "            else: \n",
        "                pass\n",
        "            del dark\n",
        "            \n",
        "            df = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_signal.parquet'))\n",
        "            fgs_signal = df.values.astype(np.float64).reshape((df.shape[0], 32, 32))\n",
        "            \n",
        "            FGS1_gain = train_adc_info['FGS1_adc_gain'].loc[index_chunk[i]]\n",
        "            FGS1_offset = train_adc_info['FGS1_adc_offset'].loc[index_chunk[i]]\n",
        "            \n",
        "            fgs_signal = ADC_convert(fgs_signal, FGS1_gain, FGS1_offset)\n",
        "            dt_fgs1 = np.ones(len(fgs_signal))*0.1\n",
        "            dt_fgs1[1::2] += 0.1\n",
        "            chopped_FGS1 = fgs_signal\n",
        "            del fgs_signal, df\n",
        "            \n",
        "            # CLEANING THE DATA: FGS1\n",
        "            flat = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_calibration/flat.parquet')).values.astype(np.float64).reshape((32, 32))\n",
        "            dark = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_calibration/dark.parquet')).values.astype(np.float64).reshape((32, 32))\n",
        "            dead_fgs1 = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_calibration/dead.parquet')).values.astype(np.float64).reshape((32, 32))\n",
        "            linear_corr = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_calibration/linear_corr.parquet')).values.astype(np.float64).reshape((6, 32, 32))\n",
        "            \n",
        "            if DO_MASK:\n",
        "                chopped_FGS1 = mask_hot_dead(chopped_FGS1, dead_fgs1, dark)\n",
        "                FGS1_clean[i] = chopped_FGS1\n",
        "            else:\n",
        "                FGS1_clean[i] = chopped_FGS1\n",
        "\n",
        "            if DO_THE_NL_CORR: \n",
        "                linear_corr_signal = apply_linear_corr(linear_corr,FGS1_clean[i])\n",
        "                FGS1_clean[i,:, :, :] = linear_corr_signal\n",
        "            del linear_corr\n",
        "            \n",
        "            if DO_DARK: \n",
        "                cleaned_signal = clean_dark(FGS1_clean[i], dead_fgs1, dark,dt_fgs1)\n",
        "                FGS1_clean[i] = cleaned_signal\n",
        "            else: \n",
        "                pass\n",
        "            del dark\n",
        "            \n",
        "        # SAVE DATA AND FREE SPACE\n",
        "        AIRS_cds = get_cds(AIRS_CH0_clean)\n",
        "        FGS1_cds = get_cds(FGS1_clean)\n",
        "        \n",
        "        del AIRS_CH0_clean, FGS1_clean\n",
        "        \n",
        "        ## (Optional) Time Binning to reduce space\n",
        "        if TIME_BINNING:\n",
        "            AIRS_cds_binned = bin_obs(AIRS_cds,binning=30)\n",
        "            FGS1_cds_binned = bin_obs(FGS1_cds,binning=30*12)\n",
        "        else:\n",
        "            AIRS_cds = AIRS_cds.transpose(0,1,3,2) ## this is important to make it consistent for flat fielding, but you can always change it\n",
        "            AIRS_cds_binned = AIRS_cds\n",
        "            FGS1_cds = FGS1_cds.transpose(0,1,3,2)\n",
        "            FGS1_cds_binned = FGS1_cds\n",
        "        \n",
        "        del AIRS_cds, FGS1_cds\n",
        "        \n",
        "        for i in range (CHUNKS_SIZE):\n",
        "            flat_airs = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/AIRS-CH0_calibration/flat.parquet')).values.astype(np.float64).reshape((32, 356))[:, cut_inf:cut_sup]\n",
        "            flat_fgs = pd.read_parquet(os.path.join(path_folder,f'train/{index_chunk[i]}/FGS1_calibration/flat.parquet')).values.astype(np.float64).reshape((32, 32))\n",
        "            if DO_FLAT:\n",
        "                corrected_AIRS_cds_binned = correct_flat_field(flat_airs,dead_airs, AIRS_cds_binned[i])\n",
        "                AIRS_cds_binned[i] = corrected_AIRS_cds_binned\n",
        "                corrected_FGS1_cds_binned = correct_flat_field(flat_fgs,dead_fgs1, FGS1_cds_binned[i])\n",
        "                FGS1_cds_binned[i] = corrected_FGS1_cds_binned\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "        ## save data\n",
        "        np.save(os.path.join(path_out, 'AIRS_clean_train_{}.npy'.format(n)), AIRS_cds_binned)\n",
        "        np.save(os.path.join(path_out, 'FGS1_train_{}.npy'.format(n)), FGS1_cds_binned)\n",
        "        del AIRS_cds_binned\n",
        "        del FGS1_cds_binned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'int' object is not iterable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcalibrate_all_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[44], line 2\u001b[0m, in \u001b[0;36mcalibrate_all_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalibrate_all_data\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n, index_chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(index)):\n\u001b[0;32m      3\u001b[0m         AIRS_CH0_clean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((CHUNKS_SIZE, \u001b[38;5;241m11250\u001b[39m, \u001b[38;5;241m32\u001b[39m, l))\n\u001b[0;32m      4\u001b[0m         FGS1_clean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((CHUNKS_SIZE, \u001b[38;5;241m135000\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m))\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tqdm\\std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
          ]
        }
      ],
      "source": [
        "calibrate_all_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "sel_planet = '1011759019'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "ariel data",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 9188054,
          "sourceId": 70367,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30761,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
